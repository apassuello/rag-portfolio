# Epic 2 RAG Configuration  
# Full Epic 2 features: Neural reranking, Graph enhancement, Analytics
# Production configuration with Ollama LLM

# Document processor for handling input files
document_processor:
  type: "hybrid_pdf"
  config:
    chunk_size: 1024
    chunk_overlap: 128

# High-performance embedder for Epic 2
embedder:
  type: "modular"
  config:
    model:
      type: "sentence_transformer"
      config:
        model_name: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
        device: "mps"
        normalize_embeddings: true
    batch_processor:
      type: "dynamic"
      config:
        initial_batch_size: 64
        max_batch_size: 256
        optimize_for_memory: false
    cache:
      type: "memory"
      config:
        max_entries: 100000
        max_memory_mb: 1024

# Epic 2 ModularUnifiedRetriever with all advanced features
retriever:
  type: "modular_unified"
  config:
    vector_index:
      type: "faiss"
      config:
        index_type: "IndexFlatIP"
        normalize_embeddings: true
        metric: "cosine"
    
    sparse:
      type: "bm25"
      config:
        k1: 1.2
        b: 0.75
        lowercase: true
        filter_stop_words: true
        stop_word_sets: ["english_common", "irrelevant_entities"]
        preserve_technical_terms: true
    
    # Epic 2 Feature: Graph Enhanced Fusion
    fusion:
      type: "graph_enhanced_rrf"
      config:
        k: 60
        weights:
          dense: 0.4
          sparse: 0.3
          graph: 0.3
        graph_enabled: true
        similarity_threshold: 0.65
        max_connections_per_document: 15
        use_pagerank: true
        pagerank_damping: 0.85
    
    # Epic 2 Feature: Neural Reranking
    reranker:
      type: "neural"
      config:
        enabled: true
        model_name: "cross-encoder/ms-marco-MiniLM-L6-v2"
        device: "mps"
        batch_size: 32
        max_length: 512
        max_candidates: 100
        models:
          default_model:
            name: "cross-encoder/ms-marco-MiniLM-L6-v2"
            device: "mps"
            batch_size: 32
            max_length: 512
        default_model: "default_model"

# Production answer generation with Ollama
answer_generator:
  type: "adaptive_modular"
  config:
    llm_client:
      type: "ollama"
      config:
        model_name: "llama3.2:3b"
        base_url: "http://localhost:11434"
        timeout: 30
        max_retries: 3
    temperature: 0.3
    max_tokens: 1024
    confidence_threshold: 0.85
    
    prompt_builder:
      type: "simple"
      config:
        max_context_length: 12000
        include_instructions: true
        citation_style: "inline"
    
    response_parser:
      type: "markdown"
      config:
        preserve_formatting: true
        extract_citations: true
    
    confidence_scorer:
      type: "semantic"
      config:
        relevance_weight: 0.4
        grounding_weight: 0.4
        quality_weight: 0.2
        min_answer_length: 20
        max_answer_length: 1000

# Global settings for Epic 2
global_settings:
  log_level: "INFO"
  cache_enabled: true
  performance_monitoring: true
  max_concurrent_requests: 8