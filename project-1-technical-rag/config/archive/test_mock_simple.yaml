# Test Configuration with Mock LLM Adapter
# Minimal configuration for testing without external dependencies

# Document processor configuration
document_processor:
  type: "modular"
  config:
    parser:
      implementation: "pymupdf"
      config:
        extract_images: false
        extract_tables: true
    chunker:
      implementation: "sentence_boundary"
      config:
        chunk_size: 512
        chunk_overlap: 50
        min_chunk_size: 100
        preserve_headings: true
    cleaner:
      implementation: "technical_content"
      config:
        remove_headers_footers: true
        normalize_whitespace: true
        preserve_code_blocks: true
        preserve_technical_terms: true

# Embedder configuration
embedder:
  type: "modular"
  config:
    model:
      type: "sentence_transformer"
      config:
        model_name: "all-MiniLM-L6-v2"
        device: "cpu"
        normalize_embeddings: true
    batch_processor:
      type: "dynamic"
      config:
        initial_batch_size: 32
        max_batch_size: 128
        optimize_for_memory: true
    cache:
      type: "memory"
      config:
        max_entries: 10000
        max_memory_mb: 1024

# Retriever configuration
retriever:
  type: "modular_unified"
  config:
    vector_index:
      type: "faiss"
      config:
        index_type: "IndexFlatIP"
        normalize_vectors: true
    sparse:
      type: "bm25"
      config:
        k1: 1.2
        b: 0.75
        lowercase: true
        preserve_technical_terms: true
        filter_stop_words: true
        stop_word_sets: ["english_common", "interrogative", "irrelevant_entities"]
    fusion:
      type: "rrf"
      config:
        k: 60
        dense_weight: 0.7
        sparse_weight: 0.3
    reranker:
      type: "identity"
      config: {}
    search_params:
      initial_k: 20
      final_k: 10
      score_threshold: 0.0

# Answer generator configuration using MockLLMAdapter
answer_generator:
  type: "adaptive_modular"
  config:
    prompt_builder:
      type: "simple"
      config:
        max_context_length: 4000
        include_instructions: true
        citation_style: "inline"
    llm_client:
      type: "mock"  # Using MockLLMAdapter
      config:
        model_name: "mock-test-model"
        response_pattern: "technical"
        include_citations: true
        simulate_errors: false
    response_parser:
      type: "markdown"
      config:
        extract_citations: true
        citation_formats: ["[Document N]", "[chunk_N]", "[N]", "[Document N, Page M]"]
        clean_formatting: true
    confidence_scorer:
      type: "semantic"
      config:
        factors:
          - name: "retrieval_score"
            weight: 0.3
          - name: "source_diversity" 
            weight: 0.2
          - name: "answer_relevance"
            weight: 0.3
          - name: "citation_coverage"
            weight: 0.2
        score_threshold: 0.7

# Global settings
global_settings:
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    handlers:
      console:
        enabled: true
      file:
        enabled: false
        path: "logs/rag_test.log"