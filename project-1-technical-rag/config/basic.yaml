# Basic RAG Configuration
# Simple retrieval without Epic 2 advanced features
# Uses MockLLMAdapter for testing without external dependencies

# Document processor for handling input files
document_processor:
  type: "hybrid_pdf"
  config:
    chunk_size: 512
    chunk_overlap: 64

# Embedding generator for converting text to vectors  
embedder:
  type: "modular"
  config:
    model:
      type: "sentence_transformer"
      config:
        model_name: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
        device: "mps"
        normalize_embeddings: true
    batch_processor:
      type: "dynamic"
      config:
        initial_batch_size: 32
        max_batch_size: 128
        optimize_for_memory: true
    cache:
      type: "memory"
      config:
        max_entries: 50000
        max_memory_mb: 512

# Basic ModularUnifiedRetriever without Epic 2 features
retriever:
  type: "modular_unified"
  config:
    vector_index:
      type: "faiss"
      config:
        index_type: "IndexFlatIP"
        normalize_embeddings: true
        metric: "cosine"
    
    sparse:
      type: "bm25"
      config:
        k1: 1.2
        b: 0.25  # Fixed: reduced from 0.75 to reduce document length bias
        lowercase: true
        filter_stop_words: true
        stop_word_sets: ["english_common"]
    
    # Basic fusion (no graph enhancement)
    fusion:
      type: "rrf"
      config:
        k: 30  # Fixed: reduced from 60 to increase score discriminative power
        weights:
          dense: 0.8  # Fixed: increased from 0.7 to favor working dense component
          sparse: 0.2  # Fixed: reduced from 0.3 to reduce impact of biased BM25
    
    # Basic reranking (no neural models)
    reranker:
      type: "identity"
      config:
        enabled: true

# Answer generation - Multiple LLM options
answer_generator:
  type: "adaptive_modular"
  config:
    # Current: Mock LLM for testing without dependencies
    llm_client:
      type: "mock"
      config:
        response_pattern: "technical"
        include_citations: true
        max_response_length: 512
        mock_delay: 0.05
    
    # Alternative 1: Local Ollama LLM (comment out mock, uncomment this)
    # llm_client:
    #   type: "ollama"
    #   config:
    #     model_name: "llama3.2:3b"
    #     base_url: "http://localhost:11434"
    #     timeout: 30
    #     max_retries: 2
    
    # Alternative 2: HuggingFace API LLM (comment out mock, uncomment this)
    # llm_client:
    #   type: "huggingface"
    #   config:
    #     model_name: "microsoft/DialoGPT-medium"
    #     api_token: "${HF_TOKEN}"  # Set HF_TOKEN environment variable
    #     timeout: 30
    #     use_chat_completion: true
    #     fallback_models:
    #       - "google/gemma-2-2b-it"
    #       - "google/flan-t5-small"
    #     max_tokens: 512
    #     temperature: 0.1
    #     top_p: 0.9
    
    temperature: 0.3
    max_tokens: 512
    confidence_threshold: 0.8
    
    prompt_builder:
      type: "simple"
      config:
        max_context_length: 8000
        include_instructions: true
        citation_style: "inline"
    
    response_parser:
      type: "markdown"
      config:
        preserve_formatting: true
        extract_citations: true
    
    confidence_scorer:
      type: "semantic"
      config:
        relevance_weight: 0.4
        grounding_weight: 0.4
        quality_weight: 0.2
        min_answer_length: 20
        max_answer_length: 1000

# Global settings
global_settings:
  log_level: "INFO"
  cache_enabled: true
  performance_monitoring: false
  max_concurrent_requests: 4