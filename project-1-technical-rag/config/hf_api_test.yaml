# HuggingFace API Test Configuration
# This configuration uses HuggingFace API for all LLM operations
# while maintaining the same Epic 2 architecture and features

# Document processor for handling input files
document_processor:
  type: "hybrid_pdf"
  config:
    chunk_size: 1024
    chunk_overlap: 128

# Embedding generator for converting text to vectors  
embedder:
  type: "modular"
  config:
    model:
      type: "sentence_transformer"
      config:
        model_name: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
        device: "auto"
        normalize_embeddings: true
    batch_processor:
      type: "dynamic"
      config:
        initial_batch_size: 64
        max_batch_size: 256
        optimize_for_memory: false
    cache:
      type: "memory"
      config:
        max_entries: 100000
        max_memory_mb: 1024

# EPIC 2 ADVANCED RETRIEVER CONFIGURATION (same as advanced_test.yaml)
retriever:
  type: "modular_unified"
  config:
    # Backend Configuration
    backends:
      primary_backend: "faiss"
      fallback_enabled: true
      fallback_backend: null
      
      # Hot-swapping configuration
      enable_hot_swap: false
      health_check_interval_seconds: 30
      switch_threshold_error_rate: 0.1
      
      # FAISS backend settings
      faiss:
        index_type: "IndexFlatIP"
        normalize_embeddings: true
        metric: "cosine"
      
      # Weaviate backend settings (disabled for testing)
      weaviate: null

    # Hybrid Search Configuration
    hybrid_search:
      enabled: true
      dense_weight: 0.4
      sparse_weight: 0.3
      graph_weight: 0.3
      fusion_method: "rrf"
      rrf_k: 60
      adaptive_weights: false
      query_dependent_weighting: false
      normalization_method: "min_max"
      max_candidates_per_strategy: 200
      early_termination_threshold: 0.95

    # Neural Reranking Configuration (Epic 2 enabled)
    neural_reranking:
      enabled: true
      model_name: "cross-encoder/ms-marco-MiniLM-L6-v2"
      model_type: "cross_encoder"
      device: "auto"
      max_candidates: 100
      batch_size: 32
      max_length: 512
      max_latency_ms: 5000
      fallback_to_fast_reranker: true
      fast_reranker_threshold: 100
      
      models:
        default_model:
          name: "cross-encoder/ms-marco-MiniLM-L6-v2"
          device: "auto"
          batch_size: 32
          max_length: 512
      default_model: "default_model"

    # Graph Retrieval Configuration (Epic 2)
    graph_retrieval:
      enabled: true
      enable_entity_linking: true
      enable_cross_references: true
      similarity_threshold: 0.65
      max_connections_per_document: 15
      use_pagerank: true
      pagerank_damping: 0.85
      use_community_detection: false
      community_algorithm: "louvain"
      max_graph_hops: 3
      graph_weight_decay: 0.5
      combine_with_vector_search: true

    # Analytics Configuration
    analytics:
      enabled: true
      collect_query_metrics: true
      collect_performance_metrics: true
      collect_quality_metrics: true
      dashboard_enabled: false
      dashboard_port: 8050
      dashboard_host: "localhost"
      auto_refresh_seconds: 5
      metrics_retention_days: 30
      detailed_logs_retention_days: 7
      enable_real_time_plots: true
      enable_query_analysis: true
      enable_performance_heatmaps: true

    # A/B Testing Configuration
    experiments:
      enabled: false
      assignment_method: "deterministic"
      assignment_key_field: "query_hash"
      min_sample_size: 100
      confidence_level: 0.95
      effect_size_threshold: 0.05
      auto_winner_detection: true
      max_experiment_duration_days: 30
      early_stopping_enabled: true

    # Legacy and Compatibility
    legacy_mode: false
    legacy_fallback: true

    # Performance Settings
    max_total_latency_ms: 700
    enable_caching: true
    cache_size: 1000

    # Feature Flags (Epic 2 enabled)
    enable_all_features: true
    feature_flags:
      weaviate_backend: false
      neural_reranking: true
      graph_retrieval: true
      analytics_dashboard: true
      ab_testing: false

# HUGGINGFACE API ANSWER GENERATION CONFIGURATION
answer_generator:
  type: "adaptive_modular"
  config:
    # HuggingFace API Configuration
    llm_client:
      type: "huggingface"  # Use HuggingFace adapter
      config:
        # HuggingFace API settings
        api_token: "${HF_TOKEN}"  # Set via environment variable
        model_name: "microsoft/DialoGPT-medium"  # Primary model
        use_chat_completion: true
        timeout: 30
        temperature: 0.3
        max_tokens: 1024
        
        # Fallback models if primary fails
        fallback_models:
          - "google/gemma-2-2b-it"
          - "Qwen/Qwen2.5-1.5B-Instruct"
          - "google/flan-t5-small"
    
    # Legacy parameters for backward compatibility
    model_name: "microsoft/DialoGPT-medium"
    api_token: "${HF_TOKEN}"
    temperature: 0.3
    max_tokens: 1024
    use_ollama: false
    ollama_url: "http://localhost:11434"
    use_inference_providers: false
    enable_adaptive_prompts: false
    enable_chain_of_thought: false
    confidence_threshold: 0.85
    
    # Enhanced prompt configuration for technical documentation
    prompt_builder:
      type: "simple"
      config:
        max_context_length: 12000
        include_instructions: true
        citation_style: "inline"
        template: |
          You are an expert technical assistant specializing in RISC-V architecture and computer systems.
          
          Context Documents:
          {context}
          
          Question: {query}
          
          Instructions:
          - Provide a comprehensive, detailed technical answer based ONLY on the provided context
          - Include technical specifications, encoding details, and implementation information when available
          - Explain concepts step-by-step with technical depth appropriate for engineers
          - Cover related concepts and connections mentioned in the context
          - Include specific examples, instruction formats, or implementation details when present
          - Be thorough and detailed - technical documentation requires comprehensive coverage
          - If multiple aspects are mentioned in the context, address all of them
          - ALWAYS include citations using [Document X] format after every factual claim
          - Every technical specification, instruction format, or implementation detail must be cited
          - Use multiple citations when information comes from several sources: [Document 1, Document 2]
          
          Answer:

# Global settings
global_settings:
  environment: "testing"
  log_level: "info"